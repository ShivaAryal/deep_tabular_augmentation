{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: RandomForest with Oversampling vs Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog I'd like to show the difference deep tabular augmentation can have when training a Random Forest on a highly biased data base. In this case, we have a look at credit card fraud, where fraud itself is is way less represented than non-fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import partial\n",
    "import mlprepare as mlp\n",
    "import deep_tabular_augmentation as dta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_PATH = 'data/creditcard.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a short look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's have a look of how many more non-fraud cases we have compared to fraud cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_in_class_occurences = df['Class'].value_counts()[0]-df['Class'].value_counts()[1]\n",
    "difference_in_class_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make use of the deep tabular augmentation we need to scale the data and then use only those cases, in which class we are interested in, in this case \"Class\" is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mlp.split_df(df, dep_var='Class', test_size=0.3, split_mode='random')\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "\n",
    "X_train_fraud = X_train_scaled[np.where(y_train==1)[0]]\n",
    "X_test_fraud = X_test_scaled[np.where(y_test==1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to work we need to put our data in a DataLoader (here I use the DataBunch Class from deep data augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dta.create_datasets(X_train_fraud, y_train.values[np.where(y_train==1)], X_test_fraud, y_test.values[np.where(y_test==1)])\n",
    "data = dta.DataBunch(*dta.create_loaders(datasets, bs=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're already good to go. We can define our Variational Encoder Architecture (here: 50->12->12->5->12->12->50) and then use the LearningRate Finder to tell us the best Learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train_fraud.shape[1]\n",
    "VAE_arch = [50, 12, 12]\n",
    "target_name = 'Class'\n",
    "target_class = 1\n",
    "df_cols = list(df.columns)\n",
    "\n",
    "model = dta.Autoencoder(D_in, VAE_arch, latent_dim=5).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = dta.customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "\n",
    "run = dta.Runner(cb_funcs=[dta.LR_Find, dta.Recorder])\n",
    "\n",
    "run.fit(100, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot(skip_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a desirable learning rate and scheduler for our learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = dta.combine_scheds([0.3, 0.7], [dta.sched_cos(0.01, 0.1), dta.sched_cos(0.1, 0.01)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "train loss is: 260695.875\n",
      "validation loss is: 81048.234375\n",
      "epoch: 100\n",
      "train loss is: 189985.546875\n",
      "validation loss is: 60324.9609375\n",
      "epoch: 150\n",
      "train loss is: 136012.046875\n",
      "validation loss is: 875778.875\n",
      "epoch: 200\n",
      "train loss is: 101500.59375\n",
      "validation loss is: 2181067.75\n",
      "epoch: 250\n",
      "train loss is: 81021.46875\n",
      "validation loss is: 1641556.0\n",
      "epoch: 300\n",
      "train loss is: 67821.7421875\n",
      "validation loss is: 1316295.625\n",
      "epoch: 350\n",
      "train loss is: 58849.52734375\n",
      "validation loss is: 1098919.125\n",
      "epoch: 400\n",
      "train loss is: 52371.15625\n",
      "validation loss is: 943453.25\n"
     ]
    }
   ],
   "source": [
    "cbfs = [partial(dta.LossTracker, show_every=50), dta.Recorder, partial(dta.ParamScheduler, 'lr', sched)]\n",
    "model = dta.Autoencoder(D_in, VAE_arch, latent_dim=20).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "run = dta.Runner(cb_funcs=cbfs)\n",
    "run.fit(400, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the created data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74522.773438</td>\n",
       "      <td>-2.734601</td>\n",
       "      <td>1.005523</td>\n",
       "      <td>-2.757621</td>\n",
       "      <td>2.059611</td>\n",
       "      <td>-1.897208</td>\n",
       "      <td>-0.624007</td>\n",
       "      <td>-2.928258</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-1.309305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411411</td>\n",
       "      <td>0.489351</td>\n",
       "      <td>-0.187566</td>\n",
       "      <td>-0.093161</td>\n",
       "      <td>-0.180062</td>\n",
       "      <td>-0.021390</td>\n",
       "      <td>0.429580</td>\n",
       "      <td>-0.112336</td>\n",
       "      <td>174.301636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71122.085938</td>\n",
       "      <td>-3.364894</td>\n",
       "      <td>1.113367</td>\n",
       "      <td>-2.830953</td>\n",
       "      <td>1.941989</td>\n",
       "      <td>-2.211236</td>\n",
       "      <td>-0.474092</td>\n",
       "      <td>-3.311430</td>\n",
       "      <td>-0.250421</td>\n",
       "      <td>-1.296847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530714</td>\n",
       "      <td>0.576042</td>\n",
       "      <td>-0.143654</td>\n",
       "      <td>-0.127971</td>\n",
       "      <td>-0.212887</td>\n",
       "      <td>-0.050346</td>\n",
       "      <td>0.267594</td>\n",
       "      <td>-0.196522</td>\n",
       "      <td>152.549332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72262.554688</td>\n",
       "      <td>-3.067988</td>\n",
       "      <td>0.994937</td>\n",
       "      <td>-2.702702</td>\n",
       "      <td>1.932057</td>\n",
       "      <td>-2.033894</td>\n",
       "      <td>-0.515574</td>\n",
       "      <td>-3.079490</td>\n",
       "      <td>-0.199522</td>\n",
       "      <td>-1.262724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414692</td>\n",
       "      <td>0.561618</td>\n",
       "      <td>-0.165115</td>\n",
       "      <td>-0.111071</td>\n",
       "      <td>-0.198514</td>\n",
       "      <td>-0.028727</td>\n",
       "      <td>0.338351</td>\n",
       "      <td>-0.172759</td>\n",
       "      <td>165.774490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73973.804688</td>\n",
       "      <td>-2.787799</td>\n",
       "      <td>1.106817</td>\n",
       "      <td>-2.899842</td>\n",
       "      <td>2.136128</td>\n",
       "      <td>-1.973441</td>\n",
       "      <td>-0.626688</td>\n",
       "      <td>-3.118646</td>\n",
       "      <td>0.066192</td>\n",
       "      <td>-1.401460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510039</td>\n",
       "      <td>0.485301</td>\n",
       "      <td>-0.177926</td>\n",
       "      <td>-0.132326</td>\n",
       "      <td>-0.191207</td>\n",
       "      <td>-0.027750</td>\n",
       "      <td>0.489111</td>\n",
       "      <td>-0.112897</td>\n",
       "      <td>167.713379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71257.742188</td>\n",
       "      <td>-3.663607</td>\n",
       "      <td>1.053864</td>\n",
       "      <td>-2.765614</td>\n",
       "      <td>1.807425</td>\n",
       "      <td>-2.234919</td>\n",
       "      <td>-0.426937</td>\n",
       "      <td>-3.167879</td>\n",
       "      <td>-0.426527</td>\n",
       "      <td>-1.141452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389872</td>\n",
       "      <td>0.609650</td>\n",
       "      <td>-0.141396</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>-0.199298</td>\n",
       "      <td>-0.034171</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>-0.238806</td>\n",
       "      <td>150.548233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  74522.773438 -2.734601  1.005523 -2.757621  2.059611 -1.897208 -0.624007   \n",
       "1  71122.085938 -3.364894  1.113367 -2.830953  1.941989 -2.211236 -0.474092   \n",
       "2  72262.554688 -3.067988  0.994937 -2.702702  1.932057 -2.033894 -0.515574   \n",
       "3  73973.804688 -2.787799  1.106817 -2.899842  2.136128 -1.973441 -0.626688   \n",
       "4  71257.742188 -3.663607  1.053864 -2.765614  1.807425 -2.234919 -0.426937   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0 -2.928258  0.020250 -1.309305  ...  0.411411  0.489351 -0.187566 -0.093161   \n",
       "1 -3.311430 -0.250421 -1.296847  ...  0.530714  0.576042 -0.143654 -0.127971   \n",
       "2 -3.079490 -0.199522 -1.262724  ...  0.414692  0.561618 -0.165115 -0.111071   \n",
       "3 -3.118646  0.066192 -1.401460  ...  0.510039  0.485301 -0.177926 -0.132326   \n",
       "4 -3.167879 -0.426527 -1.141452  ...  0.389872  0.609650 -0.141396 -0.074850   \n",
       "\n",
       "        V25       V26       V27       V28      Amount  Class  \n",
       "0 -0.180062 -0.021390  0.429580 -0.112336  174.301636      1  \n",
       "1 -0.212887 -0.050346  0.267594 -0.196522  152.549332      1  \n",
       "2 -0.198514 -0.028727  0.338351 -0.172759  165.774490      1  \n",
       "3 -0.191207 -0.027750  0.489111 -0.112897  167.713379      1  \n",
       "4 -0.199298 -0.034171  0.015338 -0.238806  150.548233      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = run.predict_df(learn, no_samples=difference_in_class_occurences, scaler=x_scaler)\n",
    "std_list = list(df[df['Class']==1][df_cols].std())\n",
    "df_fake_with_noise = run.predict_with_noise_df(learn, no_samples=difference_in_class_occurences, mu=0, sigma=std_list, scaler=x_scaler)\n",
    "df_fake_with_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75590.023438</td>\n",
       "      <td>-2.798141</td>\n",
       "      <td>1.225208</td>\n",
       "      <td>-3.052627</td>\n",
       "      <td>2.222055</td>\n",
       "      <td>-1.998127</td>\n",
       "      <td>-0.654141</td>\n",
       "      <td>-3.215112</td>\n",
       "      <td>0.090021</td>\n",
       "      <td>-1.45585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600387</td>\n",
       "      <td>0.459203</td>\n",
       "      <td>-0.170964</td>\n",
       "      <td>-0.144605</td>\n",
       "      <td>-0.177815</td>\n",
       "      <td>-0.03288</td>\n",
       "      <td>0.479426</td>\n",
       "      <td>-0.091728</td>\n",
       "      <td>161.561417</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time        V1        V2        V3        V4        V5  \\\n",
       "mean  75590.023438 -2.798141  1.225208 -3.052627  2.222055 -1.998127   \n",
       "\n",
       "            V6        V7        V8       V9  ...       V21       V22  \\\n",
       "mean -0.654141 -3.215112  0.090021 -1.45585  ...  0.600387  0.459203   \n",
       "\n",
       "           V23       V24       V25      V26       V27       V28      Amount  \\\n",
       "mean -0.170964 -0.144605 -0.177815 -0.03288  0.479426 -0.091728  161.561417   \n",
       "\n",
       "      Class  \n",
       "mean    1.0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_with_noise.describe().loc[['mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare how the built-in class_weight functionality performs vs the new approach (spoiler: if you do not use any weights the RandomForest will always predict 0). Hence, we create three dataframes: the original, the original appended with fake_data, the original appended with fake data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_df_fake = pd.concat([train_df, df_fake])\n",
    "train_df_fake_with_noise = pd.concat([train_df, df_fake_with_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier to understand, let's define the datasets on which to train and on which to assess the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_train_aug = train_df.iloc[:,:30].values, test_df.iloc[:,:30].values, train_df_fake_with_noise.iloc[:,:30].values\n",
    "y_train, y_test, y_train_aug = train_df.iloc[:,30].values, test_df.iloc[:,30].values, train_df_fake_with_noise.iloc[:,30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train model on the original data while using the differences in class occurences as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True, class_weight={0:1,1:difference_in_class_occurences}).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85295,    12],\n",
       "       [  104,    32]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = rf(X_train, y_train)\n",
    "confusion_matrix(y_test, np.round(m.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the augmented dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_aug(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85291,    16],\n",
       "       [   48,    88]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_aug = rf_aug(X_train_aug, y_train_aug)\n",
    "confusion_matrix(test_df.iloc[:,30].values, np.round(m_aug.predict(test_df.iloc[:,:30].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I think that is quite astonishing. We managed to highly increase the number of fraud cases we are able to detect. Moreover, we achieved these results without any finetuning of the model architecture and simply using the default structure of the VAE. \n",
    "\n",
    "I hope this blog shed some light on why using this approach on highly biased data is worth a shot trying.\n",
    "\n",
    "Lasse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
