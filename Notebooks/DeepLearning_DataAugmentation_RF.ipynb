{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: RandomForest with Oversampling vs Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog I'd like to show the difference deep tabular augmentation can have when training a Random Forest on a highly biased data base. In this case, we have a look at credit card fraud, where fraud itself is is way less represented than non-fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import partial\n",
    "import mlprepare as mlp\n",
    "import deep_tabular_augmentation as dta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_PATH = 'data/creditcard.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a short look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's have a look of how many more non-fraud cases we have compared to fraud cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_in_class_occurences = df['Class'].value_counts()[0]-df['Class'].value_counts()[1]\n",
    "difference_in_class_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make use of the deep tabular augmentation we need to scale the data and then use only those cases, in which class we are interested in, in this case \"Class\" is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mlp.split_df(df, dep_var='Class', test_size=0.3, split_mode='random')\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "\n",
    "X_train_fraud = X_train_scaled[np.where(y_train==1)[0]]\n",
    "X_test_fraud = X_test_scaled[np.where(y_test==1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to work we need to put our data in a DataLoader (here I use the DataBunch Class from deep data augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dta.create_datasets(X_train_fraud, y_train.values[np.where(y_train==1)], X_test_fraud, y_test.values[np.where(y_test==1)])\n",
    "data = dta.DataBunch(*dta.create_loaders(datasets, bs=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're already good to go. We can define our Variational Encoder Architecture (here: 50->12->12->5->12->12->50) and then use the LearningRate Finder to tell us the best Learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train_fraud.shape[1]\n",
    "VAE_arch = [50, 12, 12]\n",
    "target_name = 'Class'\n",
    "target_class = 1\n",
    "df_cols = list(df.columns)\n",
    "\n",
    "model = dta.Autoencoder(D_in, VAE_arch, latent_dim=5).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = dta.customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "\n",
    "run = dta.Runner(cb_funcs=[dta.LR_Find, dta.Recorder])\n",
    "\n",
    "run.fit(100, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot(skip_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a desirable learning rate and scheduler for our learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = dta.combine_scheds([0.3, 0.7], [dta.sched_cos(0.01, 0.1), dta.sched_cos(0.1, 0.01)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "train loss is: 250777.15625\n",
      "validation loss is: 89807.21875\n",
      "epoch: 100\n",
      "train loss is: 184203.078125\n",
      "validation loss is: 71237.8828125\n",
      "epoch: 150\n",
      "train loss is: 129945.9765625\n",
      "validation loss is: 81472.5078125\n",
      "epoch: 200\n",
      "train loss is: 95360.5859375\n",
      "validation loss is: 260343.4375\n",
      "epoch: 250\n",
      "train loss is: 75559.625\n",
      "validation loss is: 208241.1875\n",
      "epoch: 300\n",
      "train loss is: 63336.73828125\n",
      "validation loss is: 168084.046875\n",
      "epoch: 350\n",
      "train loss is: 55019.421875\n",
      "validation loss is: 141231.15625\n",
      "epoch: 400\n",
      "train loss is: 48957.48046875\n",
      "validation loss is: 121973.09375\n"
     ]
    }
   ],
   "source": [
    "cbfs = [partial(dta.LossTracker, show_every=50), dta.Recorder, partial(dta.ParamScheduler, 'lr', sched)]\n",
    "model = dta.Autoencoder(nn.Sequential(*dta.get_lin_layers(D_in, [50, 12, 12])),\n",
    "                     nn.Sequential(*dta.get_lin_layers_rev(D_in, [50, 12, 12])),\n",
    "                     latent_dim=5).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "run = dta.Runner(cb_funcs=cbfs)\n",
    "run.fit(400, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the created data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107547.656250</td>\n",
       "      <td>-1.175546</td>\n",
       "      <td>3.190463</td>\n",
       "      <td>-5.880084</td>\n",
       "      <td>5.172310</td>\n",
       "      <td>-0.201045</td>\n",
       "      <td>-2.110816</td>\n",
       "      <td>-2.554305</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>-2.931453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680436</td>\n",
       "      <td>-0.063858</td>\n",
       "      <td>0.295163</td>\n",
       "      <td>-0.333722</td>\n",
       "      <td>0.318315</td>\n",
       "      <td>0.305315</td>\n",
       "      <td>0.570651</td>\n",
       "      <td>0.418477</td>\n",
       "      <td>59.745529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105570.804688</td>\n",
       "      <td>-1.411785</td>\n",
       "      <td>2.465464</td>\n",
       "      <td>-4.638255</td>\n",
       "      <td>3.691925</td>\n",
       "      <td>-1.264926</td>\n",
       "      <td>-1.476137</td>\n",
       "      <td>-2.528746</td>\n",
       "      <td>0.799087</td>\n",
       "      <td>-2.038979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874525</td>\n",
       "      <td>0.115001</td>\n",
       "      <td>0.530135</td>\n",
       "      <td>-0.186685</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.213698</td>\n",
       "      <td>0.545221</td>\n",
       "      <td>0.412415</td>\n",
       "      <td>103.826714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104763.757812</td>\n",
       "      <td>-1.315135</td>\n",
       "      <td>2.079836</td>\n",
       "      <td>-3.970082</td>\n",
       "      <td>2.974885</td>\n",
       "      <td>-1.478414</td>\n",
       "      <td>-1.190014</td>\n",
       "      <td>-2.352298</td>\n",
       "      <td>0.718098</td>\n",
       "      <td>-1.687526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794227</td>\n",
       "      <td>0.149139</td>\n",
       "      <td>0.566353</td>\n",
       "      <td>-0.152397</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>0.140893</td>\n",
       "      <td>0.520099</td>\n",
       "      <td>0.307183</td>\n",
       "      <td>114.956940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106964.625000</td>\n",
       "      <td>-1.133216</td>\n",
       "      <td>2.987270</td>\n",
       "      <td>-5.564784</td>\n",
       "      <td>4.863543</td>\n",
       "      <td>-0.300885</td>\n",
       "      <td>-1.990756</td>\n",
       "      <td>-2.479523</td>\n",
       "      <td>0.771097</td>\n",
       "      <td>-2.773068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650077</td>\n",
       "      <td>-0.045114</td>\n",
       "      <td>0.288558</td>\n",
       "      <td>-0.310352</td>\n",
       "      <td>0.305301</td>\n",
       "      <td>0.276039</td>\n",
       "      <td>0.575313</td>\n",
       "      <td>0.383283</td>\n",
       "      <td>70.325348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105466.343750</td>\n",
       "      <td>-1.274740</td>\n",
       "      <td>2.423632</td>\n",
       "      <td>-4.584245</td>\n",
       "      <td>3.720120</td>\n",
       "      <td>-1.073872</td>\n",
       "      <td>-1.501573</td>\n",
       "      <td>-2.430743</td>\n",
       "      <td>0.760877</td>\n",
       "      <td>-2.088018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771964</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.460291</td>\n",
       "      <td>-0.201218</td>\n",
       "      <td>0.260977</td>\n",
       "      <td>0.201086</td>\n",
       "      <td>0.546640</td>\n",
       "      <td>0.366659</td>\n",
       "      <td>101.931236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  107547.656250 -1.175546  3.190463 -5.880084  5.172310 -0.201045 -2.110816   \n",
       "1  105570.804688 -1.411785  2.465464 -4.638255  3.691925 -1.264926 -1.476137   \n",
       "2  104763.757812 -1.315135  2.079836 -3.970082  2.974885 -1.478414 -1.190014   \n",
       "3  106964.625000 -1.133216  2.987270 -5.564784  4.863543 -0.300885 -1.990756   \n",
       "4  105466.343750 -1.274740  2.423632 -4.584245  3.720120 -1.073872 -1.501573   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0 -2.554305  0.789000 -2.931453  ...  0.680436 -0.063858  0.295163 -0.333722   \n",
       "1 -2.528746  0.799087 -2.038979  ...  0.874525  0.115001  0.530135 -0.186685   \n",
       "2 -2.352298  0.718098 -1.687526  ...  0.794227  0.149139  0.566353 -0.152397   \n",
       "3 -2.479523  0.771097 -2.773068  ...  0.650077 -0.045114  0.288558 -0.310352   \n",
       "4 -2.430743  0.760877 -2.088018  ...  0.771964  0.084755  0.460291 -0.201218   \n",
       "\n",
       "        V25       V26       V27       V28      Amount  Class  \n",
       "0  0.318315  0.305315  0.570651  0.418477   59.745529      1  \n",
       "1  0.258670  0.213698  0.545221  0.412415  103.826714      1  \n",
       "2  0.244498  0.140893  0.520099  0.307183  114.956940      1  \n",
       "3  0.305301  0.276039  0.575313  0.383283   70.325348      1  \n",
       "4  0.260977  0.201086  0.546640  0.366659  101.931236      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = run.predict_df(learn, no_samples=difference_in_class_occurences, scaler=x_scaler)\n",
    "df_fake_with_noise = run.predict_with_noise_df(learn, no_samples=difference_in_class_occurences, mu=0, sigma=0.1, scaler=x_scaler)\n",
    "df_fake_with_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>105288.890625</td>\n",
       "      <td>-1.479125</td>\n",
       "      <td>2.557713</td>\n",
       "      <td>-4.819775</td>\n",
       "      <td>3.855684</td>\n",
       "      <td>-1.171547</td>\n",
       "      <td>-1.542699</td>\n",
       "      <td>-2.576222</td>\n",
       "      <td>0.791697</td>\n",
       "      <td>-2.12214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860909</td>\n",
       "      <td>0.103987</td>\n",
       "      <td>0.481285</td>\n",
       "      <td>-0.208098</td>\n",
       "      <td>0.249638</td>\n",
       "      <td>0.217605</td>\n",
       "      <td>0.525674</td>\n",
       "      <td>0.399994</td>\n",
       "      <td>99.816528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time        V1        V2        V3        V4        V5  \\\n",
       "mean  105288.890625 -1.479125  2.557713 -4.819775  3.855684 -1.171547   \n",
       "\n",
       "            V6        V7        V8       V9  ...       V21       V22  \\\n",
       "mean -1.542699 -2.576222  0.791697 -2.12214  ...  0.860909  0.103987   \n",
       "\n",
       "           V23       V24       V25       V26       V27       V28     Amount  \\\n",
       "mean  0.481285 -0.208098  0.249638  0.217605  0.525674  0.399994  99.816528   \n",
       "\n",
       "      Class  \n",
       "mean    1.0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_with_noise.describe().loc[['mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare how the built-in class_weight functionality performs vs the new approach (spoiler: if you do not use any weights the RandomForest will always predict 0). Hence, we create three dataframes: the original, the original appended with fake_data, the original appended with fake data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_df_fake = pd.concat([train_df, df_fake])\n",
    "train_df_fake_with_noise = pd.concat([train_df, df_fake_with_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier to understand, let's define the datasets on which to train and on which to assess the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_train_aug = train_df.iloc[:,:30].values, test_df.iloc[:,:30].values, train_df_fake_with_noise.iloc[:,:30].values\n",
    "y_train, y_test, y_train_aug = train_df.iloc[:,30].values, test_df.iloc[:,30].values, train_df_fake_with_noise.iloc[:,30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train model on the original data while using the differences in class occurences as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True, class_weight={0:1,1:difference_in_class_occurences}).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85300,     7],\n",
       "       [   99,    37]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = rf(X_train, y_train)\n",
    "confusion_matrix(y_test, np.round(m.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the augmented dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_aug(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85288,    19],\n",
       "       [   46,    90]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_aug = rf_aug(X_train_aug, y_train_aug)\n",
    "confusion_matrix(test_df.iloc[:,30].values, np.round(m_aug.predict(test_df.iloc[:,:30].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I think that is quite astonishing. We managed to highly increase the number of fraud cases we are able to detect. Moreover, we achieved these results without any finetuning of the model architecture and simply using the default structure of the VAE. \n",
    "\n",
    "I hope this blog shed some light on why using this approach on highly biased data is worth a shot trying.\n",
    "\n",
    "Lasse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
